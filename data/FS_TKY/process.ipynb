{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#过滤掉签到记录少于10次的用户，被签到次数少于10次的地点，并对用户和位置重新编号\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('dataset_TKY.txt', encoding=\"unicode_escape\", names=['use_ID', 'loc_ID', 'loc_cat_ID', 'loc_cat_name', 'latitude', 'longitude', 'time_offset', 'time'], sep='\\t')\n",
    "data_exact = data[['use_ID', 'loc_ID', 'latitude', 'longitude', 'time']]\n",
    "data_exact['time'] = pd.to_datetime(data_exact['time'])\n",
    "data_exact['time'] = data_exact['time'] + pd.Timedelta(minutes=540)#将time更改为本地时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#过滤数据\n",
    "def filter(data):\n",
    "    dm = data[['use_ID', 'loc_ID']]\n",
    "    user_cnt = dm.groupby(by=['use_ID'], as_index=False).count()\n",
    "    user_list = list(user_cnt[user_cnt['loc_ID'] >= 10]['use_ID'])#过滤掉签到次数少于10次的用户\n",
    "    ds = data[data['use_ID'].isin(user_list)]\n",
    "    loc_cnt = ds.groupby(by=['loc_ID'], as_index=False).count()\n",
    "    loc_list = list(loc_cnt[loc_cnt['use_ID'] >= 10]['loc_ID'])#过滤掉签到人数少于10人次的位置\n",
    "    data_filtered = ds[ds['loc_ID'].isin(loc_list)]\n",
    "    return data_filtered\n",
    "data_filtered = filter(data_exact)\n",
    "user_cnt = data_filtered.groupby(by=['use_ID'], as_index=False).count()\n",
    "control = True\n",
    "while control:\n",
    "    if (any(user_cnt['loc_ID'] < 10)):\n",
    "        data_filtered = filter(data_filtered)\n",
    "        user_cnt = data_filtered.groupby(by=['use_ID'], as_index=False).count()\n",
    "    else:\n",
    "        control = False\n",
    "data_filtered.sort_values(by='time', inplace=True)#按时间排序\n",
    "data_filtered.index = range(len(data_filtered))\n",
    "data_filtered.to_csv('FourSquare_TKY.csv', sep=',', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_n = data_exact['loc_ID'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据集划分：按时序，8:1:1（细节：在分割点处，保证一天的完整轨迹）\n",
    "import pandas as pd\n",
    "data = pd.read_csv('FourSquare_TKY.csv', names=['use_ID', 'loc_ID', 'latitude', 'longitude', 'time'], sep=',', header=0)\n",
    "data['time'] = pd.to_datetime(data['time'])\n",
    "train = data.head(int(0.8*447570+122))#+122：将分割时间点所在的一天的签到记录归纳到训练集或验证集\n",
    "tmp = data.head(int(0.9*447570+409))\n",
    "valid = tmp.tail(403222-358178)\n",
    "test = data.tail(447570-403222)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将在训练集中没有而在验证集中出现的用户ID或者位置ID的相关签到记录排除\n",
    "train_user = set(train['use_ID'])\n",
    "val_user = set(valid['use_ID'])\n",
    "test_user = set(test['use_ID'])\n",
    "train_loc = set(train['loc_ID'])\n",
    "val_loc = set(valid['loc_ID'])\n",
    "test_loc = set(test['loc_ID'])\n",
    "val_g = valid.groupby(by='use_ID')\n",
    "val_train_user = val_user - train_user\n",
    "for use_id, valg in val_g:\n",
    "    if use_id in val_train_user:\n",
    "        valid.drop(valg.index, inplace=True)\n",
    "val_g = valid.groupby(by='loc_ID')\n",
    "val_train_loc = val_loc - train_loc\n",
    "for loc_id, valg in val_g:\n",
    "    if loc_id in val_train_loc:\n",
    "        valid.drop(valg.index, inplace=True)\n",
    "        \n",
    "#将在训练集中没有而在测试集中出现的用户ID或者位置ID的相关签到记录排除\n",
    "test_g = test.groupby(by='use_ID')\n",
    "test_train_user = test_user - train_user\n",
    "for use_id, valg in test_g:\n",
    "    if use_id in test_train_user:\n",
    "        test.drop(valg.index, inplace=True)\n",
    "test_g = test.groupby(by='loc_ID')\n",
    "test_train_loc = test_loc - train_loc\n",
    "for loc_id, valg in test_g:\n",
    "    if loc_id in test_train_loc:\n",
    "        test.drop(valg.index, inplace=True)\n",
    "train.to_csv('train.csv', sep=',', index=False, header=True)\n",
    "valid.to_csv('valid.csv', sep=',', index=False, header=True)\n",
    "test.to_csv('test.csv', sep=',', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#处理后，验证集和测试集中出现的用户和位置均在训练集中出现过\n",
    "#可以对训练集中的用户和位置重新编号\n",
    "data = pd.read_csv('train.csv', names=['use_ID', 'loc_ID', 'latitude', 'longitude', 'time'], sep=',', header=0)\n",
    "user = list(set(data['use_ID']))\n",
    "user_id = {}\n",
    "for i, user in enumerate(user):\n",
    "    user_id[user] = i  #用户新编号\n",
    "user_id = pd.DataFrame(list(user_id.items()))\n",
    "user_id.columns = ['old_id', 'new_id']\n",
    "user_id.to_csv('user_id.csv', sep=',', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#存在位置ID一致，但是经纬度坐标不一致的情况，由于要对地理空间影响显式建模，因此需要做一定处理，暂采用均值处理\n",
    "import pandas as pd\n",
    "data = pd.read_csv('train.csv', names=['use_ID', 'loc_ID', 'latitude', 'longitude', 'time'], sep=',', header=0)\n",
    "loc = data[['loc_ID', 'latitude', 'longitude']].drop_duplicates()\n",
    "loc_g = loc.groupby(by=['loc_ID'], as_index=False).count()\n",
    "multi_loc = loc_g[loc_g['latitude']>=2]#提取loc_ID一致，但是经纬度不一致的数据\n",
    "multi_loc.index = range(len(multi_loc))\n",
    "multi_loc_item = loc[loc['loc_ID'].isin(multi_loc['loc_ID'])]\n",
    "loc_only_coor = pd.DataFrame()\n",
    "for i in range(len(multi_loc)):\n",
    "    a = multi_loc_item[multi_loc_item['loc_ID'].isin([multi_loc['loc_ID'][i]])].mean()#经纬度各自求均值\n",
    "    loc_only_coor = pd.concat([loc_only_coor,a],axis=1)\n",
    "loc_only_coor = loc_only_coor.T\n",
    "loc_only_coor.index = range(len(loc_only_coor))\n",
    "loc_only_coor['loc_ID'] = multi_loc['loc_ID']\n",
    "loc_only = loc_only_coor[['loc_ID', 'latitude', 'longitude']]\n",
    "loc_del_multi = loc[~loc['loc_ID'].isin(multi_loc['loc_ID'])]\n",
    "loc_global = pd.concat([loc_del_multi, loc_only])\n",
    "loc_global.index = range(len(loc_global))\n",
    "loc_global['loc_new_ID'] = loc_global.index #赋予新ID\n",
    "loc_global.to_csv('loc.csv', sep=',', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将训练集、验证集、测试集中的用户ID和位置ID替换成新ID\n",
    "import pandas as pd\n",
    "data = pd.read_csv('test.csv', names=['use_ID', 'loc_ID', 'latitude', 'longitude', 'time'], sep=',', header=0)\n",
    "loc = pd.read_csv('loc.csv', names=['loc_ID', 'latitude', 'longitude', 'loc_new_ID'], sep=',', header=0)\n",
    "user = pd.read_csv('user_id.csv', names=['old_id', 'new_id'], sep=',', header=0)\n",
    "user_id = {}\n",
    "loc_id = {}\n",
    "for i in range(len(user)):\n",
    "    user_id[user['old_id'][i]] = user['new_id'][i]\n",
    "for i in range(len(loc)):\n",
    "    loc_id[loc['loc_ID'][i]] = loc['loc_new_ID'][i]\n",
    "data['user_new_ID'] = -1\n",
    "data['loc_new_ID'] = -1\n",
    "for i in range(len(data)):\n",
    "    data['user_new_ID'][i] = user_id[data['use_ID'][i]]  \n",
    "    data['loc_new_ID'][i] = loc_id[data['loc_ID'][i]] \n",
    "train_data = data[['user_new_ID', 'loc_new_ID', 'time']]\n",
    "train_data.to_csv('TKY_test.csv', sep=',', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将训练集、验证集、测试集签到数据按用户，连续两次签到间隔超过24小时，拆分为不同的轨迹\n",
    "import pandas as pd\n",
    "import json\n",
    "data = pd.read_csv('TKY_train.csv', names=['user_new_ID', 'loc_new_ID', 'time'], sep=',', header=0)\n",
    "data.sort_values(['user_new_ID','time'], ascending=[True, True], inplace=True)\n",
    "data.index = range(len(data))\n",
    "#生成每条签到记录的轨迹号\n",
    "data['time'] = pd.to_datetime(data['time'])\n",
    "data_g = data.groupby(by='user_new_ID')\n",
    "data_traj = pd.DataFrame()\n",
    "for user, datag in data_g:\n",
    "    traj = 0\n",
    "    datag['traj_num'] = -1\n",
    "    datag.index = range(len(datag))\n",
    "    f = 0\n",
    "    for i in range(len(datag)-1):\n",
    "        deltatime = datag['time'][i+1] - datag['time'][i]\n",
    "        if deltatime.days >= 1:\n",
    "            datag['traj_num'][f:i+1] = traj\n",
    "            f = i + 1\n",
    "            traj += 1\n",
    "    datag['traj_num'][f:] = traj\n",
    "    data_traj = pd.concat((data_traj, datag))\n",
    "data_traj.index = range(len(data_traj))\n",
    "#生成每条签到记录所在的星期几名字\n",
    "week = [data_traj['time'][i].isoweekday() for i in range(len(data_traj))]\n",
    "data_traj['day_of_week'] = week\n",
    "#生成每条签到记录的时间片\n",
    "tm = data_traj['time']\n",
    "tms = list(map(lambda x: x.hour * 2 + 2 if x.minute >=30 else x.hour * 2 + 1, tm))\n",
    "data_traj['timeslot'] = pd.Series(tms)\n",
    "data_traj.to_csv('train_traj.csv', sep=',', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#提取轨迹序列\n",
    "import numpy as np\n",
    "data_traj_g = data_traj.groupby(by=['user_new_ID', 'traj_num'])\n",
    "for i, traj in data_traj_g:\n",
    "    d_t = traj[['loc_new_ID', 'timeslot', 'day_of_week']]\n",
    "    if len(d_t) > 1:\n",
    "       d_t = np.array(d_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_traj_g = data_traj.groupby(by=['user_new_ID'])\n",
    "traj_all = []\n",
    "# traj_all = dict()\n",
    "for i, u_traj in data_traj_g:\n",
    "    u_traj_g = u_traj.groupby(by=['traj_num'])\n",
    "    traj_src = dict()\n",
    "    traj_pred = dict()\n",
    "    for n, d_traj in u_traj_g:\n",
    "        d_t = d_traj[['loc_new_ID', 'timeslot', 'day_of_week']]\n",
    "        if len(d_t) > 1: #排除只有一个轨迹点的轨迹\n",
    "            traj_points = [(d_t.iloc[j][0], d_t.iloc[j][1], d_t.iloc[j][2]) for j in range(len(d_t))]\n",
    "        # if len(traj_points) > 1:\n",
    "            traj_src[n] = traj_points[:-1]\n",
    "            traj_pred[n] = traj_points[-1]\n",
    "    # traj_all[i] = {'forward': traj_src, 'pred': traj_pred}\n",
    "    if len(traj_pred) > 0:\n",
    "        traj_all.append({i: {'forward': traj_src, 'pred': traj_pred}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#提取轨迹内的转移边，构造全局转移图 \n",
    "import pandas as pd\n",
    "data = pd.read_csv('TKY_train.csv', names=['user_new_ID', 'loc_new_ID', 'time'], sep=',', header=0)\n",
    "data.sort_values(['user_new_ID','time'], ascending=[True, True], inplace=True)\n",
    "data.index = range(len(data))\n",
    "data['time'] = pd.to_datetime(data['time'])\n",
    "data_g = data.groupby(by='user_new_ID')\n",
    "data_traj = pd.DataFrame()\n",
    "#提取用户的各条轨迹\n",
    "for user, datag in data_g:\n",
    "    traj = 0\n",
    "    datag['traj_num'] = -1\n",
    "    datag.index = range(len(datag))\n",
    "    f = 0\n",
    "    for i in range(len(datag)-1):\n",
    "        deltatime = datag['time'][i+1] - datag['time'][i]\n",
    "        if deltatime.days >= 1:\n",
    "            datag['traj_num'][f:i+1] = traj\n",
    "            f = i + 1\n",
    "            traj += 1\n",
    "    datag['traj_num'][f:] = traj\n",
    "    data_traj = pd.concat((data_traj, datag))\n",
    "data_traj.index = range(len(data_traj))\n",
    "\n",
    "#提取轨迹内的转移边：共274667条转移连边（训练集）\n",
    "data_traj_g = data_traj.groupby(by=['user_new_ID', 'traj_num'])\n",
    "edge_trans = []\n",
    "for i, traj in data_traj_g:\n",
    "    traj.index = range(len(traj))\n",
    "    trans_e = [(traj['loc_new_ID'][j], traj['loc_new_ID'][j+1]) for j in range(len(traj) - 1)]\n",
    "    edge_trans += trans_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#统计转移边及其频次 有向边：src->dst：频次\n",
    "from collections import Counter\n",
    "import json\n",
    "edge_trans_cnt = Counter(edge_trans)#最大频次为623\n",
    "global_tran_e = dict(edge_trans_cnt)\n",
    "global_tran_edge = pd.DataFrame(global_tran_e.keys())\n",
    "global_tran_edge.columns = ['src', 'dst']\n",
    "global_tran_edge['freq'] = global_tran_e.values()\n",
    "gtedge_g = global_tran_edge.groupby(by='dst')\n",
    "tran_edge = pd.DataFrame()\n",
    "for dst, e in gtedge_g:\n",
    "    e.index = range(len(e))\n",
    "    total_freq = e['freq'].sum()\n",
    "    e['weight'] = e['freq'] / total_freq\n",
    "    tran_edge = pd.concat((tran_edge, e))\n",
    "    tran_edge.to_csv('tran_edge.csv', sep=',', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建位置的地理空间连边\n",
    "import pandas as pd\n",
    "import transbigdata as tbd\n",
    "import geohash as gh\n",
    "from haversine import haversine\n",
    "data = pd.read_csv('loc.csv', names=['loc_ID', 'latitude', 'longitude', 'loc_new_ID'], sep=',', header=0)\n",
    "loc = data[['loc_new_ID', 'latitude', 'longitude']]\n",
    "geohash = tbd.geohash_encode(loc['longitude'], loc['latitude'], precision=5)#precision:5(4.89*4.89km),6(1.22*0.61km)\n",
    "geo_edge = []\n",
    "for i in range(len(loc)):\n",
    "    neigh = gh.neighbors(geohash[i])\n",
    "    for k in neigh:\n",
    "        loc_geo = geohash[geohash.values == neigh[k]]\n",
    "        if len(loc_geo) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            # for j in loc_geo.index:\n",
    "            #     dist = haversine((loc['latitude'][i], loc['longitude'][i]), (loc['latitude'][j], loc['longitude'][j]), unit='m')\n",
    "            #     if dist <= 1000:#超参：距离小于1000m的两个位置建立连边\n",
    "            #         geo_edge +=[(i, j)]\n",
    "            geo_edge += [(i, j) for j in loc_geo.index if haversine((loc['latitude'][i], loc['longitude'][i]), (loc['latitude'][j], loc['longitude'][j]), unit='m')<=1000]\n",
    "geo_edge = pd.DataFrame(geo_edge)\n",
    "geo_edge.columns = ['src', 'dst']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_edge.to_csv('geo_edge.csv', sep=',', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "traj_data = pd.read_csv('train_traj.csv', names=['user_new_ID','loc_new_ID','time','traj_num','day_of_week','timeslot'], sep=',', header=0)\n",
    "data_traj_g = traj_data.groupby(by=['user_new_ID', 'traj_num'])\n",
    "traj_f = []\n",
    "traj_p = []\n",
    "traj_user = []\n",
    "traj_user_traj_n = []\n",
    "for i, traj in data_traj_g:\n",
    "    d_t = np.array(traj[['loc_new_ID', 'timeslot', 'day_of_week']])\n",
    "    if len(d_t) > 1:#剔除只有一条签到记录的轨迹\n",
    "        traj_tmp = [tuple(d_t[i]) for i in range(len(d_t))]\n",
    "        traj_f.append(traj_tmp[:-1])\n",
    "        # traj_p.append(traj_tmp[-1])#只提取最后一次\n",
    "        traj_p.append(traj_tmp[1:])#从第2个签到记录开到最后\n",
    "        traj_user.append(i[0])\n",
    "        traj_user_traj_n.append(i[1])\n",
    "file = open('train_forward.pickle','wb')\n",
    "pickle.dump(traj_f,file)\n",
    "file.close()\n",
    "file = open('train_labels.pickle','wb')\n",
    "pickle.dump(traj_p,file)\n",
    "file.close()\n",
    "file = open('train_user.pickle','wb')\n",
    "pickle.dump(traj_user,file)\n",
    "file.close()\n",
    "# traj_p = pd.DataFrame(traj_p)\n",
    "# traj_p.columns = ['loc_new_ID', 'timeslot', 'day_of_week']\n",
    "# traj_user = pd.DataFrame(traj_user)\n",
    "# traj_user.columns = ['user_new_ID']\n",
    "# traj_pred = pd.concat((traj_user, traj_p), 1)\n",
    "# traj_pred.to_csv('valid_pred.csv', sep=',', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_file = open('train_forward.pickle','rb')\n",
    "list2 = pickle.load(list_file)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f1033eb28085bb0a582f96e806060dbf3f27ac1b7abb68d3449d2cbe540e0310"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('Ada-gat')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f1033eb28085bb0a582f96e806060dbf3f27ac1b7abb68d3449d2cbe540e0310"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
